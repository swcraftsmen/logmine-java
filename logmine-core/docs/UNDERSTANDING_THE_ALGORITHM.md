# Understanding the LogMine Algorithm

Visual, step-by-step explanation with examples.

---

## ğŸ“„ Research Paper

This algorithm is based on:

**Hamooni, H., Debnath, B., Xu, J., Zhang, H., Jiang, G., & Mueen, A. (2016).** *LogMine: Fast Pattern Recognition for Log Analytics.* CIKM 2016. 

ğŸ“ **Paper:** [https://www.cs.unm.edu/~mueen/Papers/LogMine.pdf](https://www.cs.unm.edu/~mueen/Papers/LogMine.pdf)

---

## ğŸ¯ THE BIG PICTURE

```
INPUT: Thousands of messy log messages
   â†“
STEP 1: Group similar logs together (CLUSTERING)
   â†“
STEP 2: Find common patterns in each group (PATTERN EXTRACTION)
   â†“
STEP 3: Rank patterns by frequency (SORTING)
   â†“
OUTPUT: Clean, organized patterns
```

---

## ğŸ“š STEP 1: CLUSTERING (Grouping Similar Logs)

### What is it?
Putting similar log messages into the same bucket.

### How does it work?

**Input:**
```
Log 1: "2015-07-09 10:22:12 INFO user=john action=login"
Log 2: "2015-07-09 14:35:01 INFO user=alice action=login"
Log 3: "2015-07-09 16:45:23 ERROR database connection failed"
Log 4: "2015-07-09 18:12:45 INFO user=bob action=login"
```

**Process:** Compare each log to existing clusters

```
ğŸ“¦ Cluster 1: LOGIN EVENTS
   â”œâ”€ Log 1: INFO user=john action=login
   â”œâ”€ Log 2: INFO user=alice action=login
   â””â”€ Log 4: INFO user=bob action=login

ğŸ“¦ Cluster 2: DATABASE ERRORS
   â””â”€ Log 3: ERROR database connection failed
```

**Key Concept: SIMILARITY**
- Logs 1, 2, 4 are similar (same structure, different usernames)
- Log 3 is different (completely different message)

### The "Edit Distance" Magic

**Question:** How similar are these?
```
Log A: "INFO user=john"
Log B: "INFO user=alice"
```

**Answer:** Count how many changes needed:
- "john" â†’ "alice" = 1 change
- Everything else is identical
- Similarity = Very High! âœ…

**Compare to:**
```
Log A: "INFO user=john"
Log C: "ERROR database failed"
```
- Need to change: INFOâ†’ERROR, userâ†’database, johnâ†’failed
- Similarity = Very Low! âŒ

---

## ğŸ” STEP 2: PATTERN EXTRACTION (Finding the Template)

### What is it?
For each cluster, identify what STAYS THE SAME vs what CHANGES.

### Example:

**Cluster Contents:**
```
1. "2015-07-09 10:22:12 INFO user=john action=login"
2. "2015-07-09 14:35:01 INFO user=alice action=login"
3. "2015-07-09 18:12:45 INFO user=bob action=login"
```

**Analysis (token by token):**
```
Position 0: "2015-07-09"  "2015-07-09"  "2015-07-09"  â†’ DIFFERENT dates â†’ ***
Position 1: "10:22:12"    "14:35:01"    "18:12:45"    â†’ DIFFERENT times â†’ ***
Position 2: "INFO"        "INFO"        "INFO"        â†’ SAME! â†’ Keep "INFO"
Position 3: "user=john"   "user=alice"  "user=bob"    â†’ DIFFERENT users â†’ ***
Position 4: "action=login" "action=login" "action=login" â†’ SAME! â†’ Keep "action=login"
```

**Extracted Pattern:**
```
*** *** INFO user=*** action=login
```

**What this means:**
- `***` = Variable part (changes)
- `INFO`, `action=login` = Constant part (always there)

---

## ğŸ“Š STEP 3: RANKING (Sorting by Importance)

### What is it?
Patterns that appear MORE often are MORE important.

### Example:

```
Pattern A: "*** INFO user=*** action=login"     â†’ Appears 1,000 times
Pattern B: "*** ERROR database connection"       â†’ Appears 50 times  
Pattern C: "*** WARNING disk space low"          â†’ Appears 5 times
```

**Sorted Output:**
```
1. "*** INFO user=*** action=login"     (support: 1000) â† Most common
2. "*** ERROR database connection"      (support: 50)
3. "*** WARNING disk space low"         (support: 5)
```

---

## ğŸ›ï¸ CONFIGURATION: The Knobs You Can Turn

### 1. **Similarity Threshold** (0.0 to 1.0)

Controls how strict clustering is:

```
THRESHOLD = 0.9 (Very Strict)
â”œâ”€ Only VERY similar logs cluster together
â”œâ”€ Result: MANY clusters, MANY specific patterns
â””â”€ Use when: You know your log format

THRESHOLD = 0.5 (Balanced) â† DEFAULT
â”œâ”€ Reasonably similar logs cluster together
â”œâ”€ Result: Moderate clusters, useful patterns
â””â”€ Use when: Mixed log sources

THRESHOLD = 0.3 (Very Loose)
â”œâ”€ Even somewhat different logs cluster together
â”œâ”€ Result: FEW clusters, GENERAL patterns
â””â”€ Use when: Very diverse, unknown sources
```

**Visual Example:**

```
With Threshold = 0.8 (Strict):
ğŸ“¦ Cluster 1: "INFO user login"
ğŸ“¦ Cluster 2: "INFO user logout"
ğŸ“¦ Cluster 3: "WARNING user login"
â†’ 3 separate patterns (very specific)

With Threshold = 0.4 (Loose):
ğŸ“¦ Cluster 1: "*** user ***"
â†’ 1 general pattern (covers all above)
```

### 2. **Minimum Cluster Size**

Filters out rare/noisy patterns:

```
MIN_SIZE = 1
â”œâ”€ Keep patterns that appear even once
â””â”€ Use for: Anomaly detection, finding rare events

MIN_SIZE = 3
â”œâ”€ Only keep patterns appearing 3+ times
â””â”€ Use for: Normal monitoring, filter noise

MIN_SIZE = 10
â”œâ”€ Only keep frequent patterns
â””â”€ Use for: High-traffic systems, reliable alerts
```

### 3. **Normalization** (Pre-processing)

Replace common variable parts BEFORE clustering:

```
BEFORE Normalization:
- "Request from 192.168.1.100"
- "Request from 192.168.1.101"
- "Request from 10.0.0.50"
â†’ These look DIFFERENT (different IPs)
â†’ Result: 3 separate patterns

AFTER IP Normalization:
- "Request from <IP>"
- "Request from <IP>"
- "Request from <IP>"
â†’ These look IDENTICAL
â†’ Result: 1 pattern! âœ…
```

**Types of Normalization:**
- `normalizeTimestamps`: `2015-07-09 10:22:12` â†’ `<TIMESTAMP>`
- `normalizeIPs`: `192.168.1.1` â†’ `<IP>`
- `normalizeNumbers`: `1234` â†’ `<NUM>`
- `normalizePaths`: `/usr/local/bin` â†’ `<PATH>`
- `normalizeUrls`: `http://example.com` â†’ `<URL>`

---

## ğŸŒ² HIERARCHICAL PATTERNS

### What is it?
Extract patterns at MULTIPLE levels of detail simultaneously.

### Example:

**Same Logs, Different Thresholds:**

```
Level 1 (Threshold = 0.8) - VERY SPECIFIC:
1. "*** INFO [worker-1] Processing request from user ***"
2. "*** INFO [worker-2] Processing request from user ***"
3. "*** INFO [worker-3] Processing request from user ***"
â†’ 3 patterns (distinguishes worker threads)

Level 2 (Threshold = 0.5) - MODERATE:
1. "*** INFO [***] Processing request from user ***"
â†’ 1 pattern (worker thread becomes wildcard)

Level 3 (Threshold = 0.3) - GENERAL:
1. "*** INFO [***] *** *** *** user ***"
â†’ 1 very general pattern
```

**When to use each level:**
- **Level 1 (Specific)**: Debugging, finding exact issues
- **Level 2 (Moderate)**: Dashboards, monitoring
- **Level 3 (General)**: High-level overview, reporting

---

## ğŸ’¡ PRACTICAL USE CASES

### Use Case 1: Unknown Log Sources

**Problem:** You receive logs from 10 different systems, don't know their formats.

**Solution:**
```java
// Use multi-source config with lenient threshold
LogMineProcessor processor = new LogMineProcessor(0.5, 2);
List<LogPattern> patterns = processor.process(allLogs);

// LogMine automatically discovers ALL formats!
```

### Use Case 2: Anomaly Detection

**Problem:** Find unusual/suspicious log messages.

**Solution:**
```java
// Train on normal logs
LogMineProcessor processor = new LogMineProcessor(0.6, 3);
processor.process(normalLogs);

// Test new logs
for (String log : newLogs) {
    if (processor.matchPattern(log) == null) {
        alert("ANOMALY: " + log);
    }
}
```

### Use Case 3: Log Compression

**Problem:** Store 1 million logs efficiently.

**Solution:**
```java
// Extract patterns
processor.process(millionLogs);

// Store just patterns + counts
// "*** INFO user=*** action=login" â†’ 500,000 occurrences
// "*** ERROR database timeout" â†’ 1,000 occurrences

// Saved 99% space! âœ…
```

---

## ğŸš€ QUICK START

### For Unknown/Mixed Sources:

```java
// Just set threshold and min size
LogMineProcessor processor = new LogMineProcessor(0.5, 2);
List<LogPattern> patterns = processor.process(yourLogs);

// That's it! LogMine figures out the rest.
```

### For Multiple Detail Levels:

```java
// Try 3 thresholds
double[] thresholds = {0.8, 0.5, 0.3};

for (double t : thresholds) {
    LogMineProcessor p = new LogMineProcessor(t, 2);
    List<LogPattern> patterns = p.process(logs);
    System.out.println("Threshold " + t + ": " + patterns.size() + " patterns");
}

// Pick the level that works best for you!
```

---

## ğŸ“ KEY TAKEAWAYS

1. **No Prior Knowledge Needed**
   - Don't need to know log formats
   - Don't need to write regexes
   - Just provide logs!

2. **Two Main Knobs**
   - Similarity Threshold (0.3-0.8)
   - Minimum Cluster Size (1-10)

3. **Optional Enhancements**
   - Normalization (helps clustering)
   - Hierarchical patterns (multiple detail levels)

4. **Start Simple, Tune Later**
   - Begin with defaults (0.5 threshold, size 2)
   - Too many patterns? Increase threshold
   - Too few patterns? Decrease threshold

---

## ğŸ“– Further Reading

- Full paper: https://www.cs.unm.edu/~mueen/Papers/LogMine.pdf
- Configuration guide: See `CONFIGURATION.md`
- Multi-source demo: Run `MultiSourceDemo.java`

**Remember:** LogMine is designed to be UNSUPERVISED and CONFIGURABLE. You don't need to understand every detailâ€”just tune 1-2 parameters and it works! ğŸ‰

